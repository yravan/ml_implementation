"""
Optimized Linear Layer
======================

Drop-in replacement for Linear with fused forward/backward.

Optimizations:
    1. Fused matmul + bias â€” eliminates Add graph node (fewer backward calls)
    2. Pre-allocated weight gradient buffer â€” avoids 144MB malloc each backward
    3. Single Function node instead of MatMul + Add â€” reduces graph traversal
    4. Optional: direct Accelerate/BLAS calls via ctypes (macOS)
"""

import numpy as np
from typing import Optional

from .init import xavier_normal_, kaiming_normal_, zeros_, normal_
from .module import Module, Parameter
from python.foundations import Tensor, Function, _no_grad


class LinearOp(Function):
    """
    Fused linear operation: y = x @ weight [+ bias]

    Single backward computes all three gradients (dx, dw, db) in one call,
    eliminating the separate Add node for bias.

    Pre-allocates weight gradient buffer to avoid large malloc on each backward.
    """

    def forward(self, x, weight, bias=None, _grad_buf=None):
        global _no_grad
        out = x @ weight
        if bias is not None:
            out += bias
        if not _no_grad:
            self.x = x
            self.weight = weight
            self.has_bias = bias is not None
            self._grad_buf = _grad_buf
        return out

    def backward(self, grad_output):
        # grad_input: (B, out) @ (out, in) = (B, in)
        grad_x = grad_output @ self.weight.T

        # grad_weight: (in, B) @ (B, out) = (in, out)
        # Use pre-allocated buffer if available to avoid large malloc
        if self._grad_buf is not None and self._grad_buf.shape == self.weight.shape:
            np.matmul(self.x.T, grad_output, out=self._grad_buf)
            grad_weight = self._grad_buf
        else:
            grad_weight = self.x.T @ grad_output

        if self.has_bias:
            grad_bias = grad_output.sum(axis=0)
            return grad_x, grad_weight, grad_bias

        return grad_x, grad_weight


# Manual Tensor wrapper (like convert_to_function but handles optional bias
# and pre-allocated buffer)
def linear_op(x_tensor, weight_tensor, bias_tensor=None, _grad_buf=None):
    """Functional interface: Tensor in, Tensor out."""
    fn = LinearOp()
    children = [x_tensor, weight_tensor]
    args = [x_tensor.data, weight_tensor.data]

    bias_data = None
    if bias_tensor is not None:
        children.append(bias_tensor)
        bias_data = bias_tensor.data

    requires_grad = any(c.requires_grad for c in children)
    out_data = fn.forward(*args, bias=bias_data, _grad_buf=_grad_buf)
    return Tensor(
        data=out_data,
        requires_grad=requires_grad,
        _children=tuple(children),
        _grad_fn=fn,
    )


class Linear(Module):
    """
    Optimized Linear transformation: y = xW + b

    Drop-in replacement for the standard Linear layer.
    Uses fused LinearOp for fewer graph nodes and pre-allocated buffers.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        init: str = 'normal'
    ):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(np.zeros((in_features, out_features)))
        if bias:
            self.bias = Parameter(np.zeros((out_features,)))
        else:
            self.bias = None

        # Pre-allocate weight gradient buffer (reused across backward calls)
        self._grad_buf = np.empty((in_features, out_features), dtype=np.float32)

        if init == 'xavier':
            self._init_parameters(xavier_normal_)
        elif init == 'kaiming':
            self._init_parameters(kaiming_normal_)
        elif init == 'normal':
            self._init_parameters(normal_)
        elif init == 'zeros':
            self._init_parameters(zeros_)
        else:
            raise ValueError(f"Unknown init method: {init}")

    def forward(self, x: Tensor) -> Tensor:
        return linear_op(x, self.weight, self.bias, _grad_buf=self._grad_buf)

    def extra_repr(self) -> str:
        return f"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}"


class Bilinear(Module):
    """
    Bilinear transformation: y = x1^T W x2 + b

    Applies a bilinear transformation to two inputs.
    Output[k] = x1^T @ W[k] @ x2 + b[k]

    Used in:
    - Attention mechanisms
    - Relation modeling
    - Low-rank approximations

    Attributes:
        weight: Parameter tensor of shape (out_features, in1_features, in2_features)
        bias: Parameter vector of shape (out_features,)

    Example:
        >>> bilinear = Bilinear(64, 32, 10)
        >>> x1 = Tensor(np.random.randn(8, 64))  # (batch, in1)
        >>> x2 = Tensor(np.random.randn(8, 32))  # (batch, in2)
        >>> y = bilinear(x1, x2)  # (batch, out)
    """

    def __init__(
        self,
        in1_features: int,
        in2_features: int,
        out_features: int,
        bias: bool = True,
        init: str = 'xavier'
    ):
        """
        Initialize Bilinear layer.

        Args:
            in1_features: Size of first input
            in2_features: Size of second input
            out_features: Size of output
            bias: Whether to include bias
        """
        super().__init__()
        self.weight = Parameter(np.zeros((out_features, in1_features, in2_features)))
        if bias:
            self.bias = Parameter(np.zeros((out_features,)))
        else:
            self.bias = None
        if init == 'xavier':
            self._init_parameters(xavier_normal_)
        elif init == 'kaiming':
            self._init_parameters(kaiming_normal_)
        elif init == 'normal':
            self._init_parameters(normal_)
        elif init == 'zeros':
            self._init_parameters(zeros_)
        else:
            raise ValueError(f"Unknown init method: {init}")

    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:
        """
        Forward pass.

        Args:
            x1: First input Tensor, shape (batch, in1_features)
            x2: Second input Tensor, shape (batch, in2_features)

        Returns:
            Output Tensor, shape (batch, out_features)

        Note:
            Backward is automatic via the computational graph.
        """
        x1 = x1.reshape(*x1.shape[:-1], 1, -1)
        x2 = x2.reshape(*x1.shape[:-1], 1, -1)
        out = (((x1 @ self.weight) * x2).sum(axis=-1))
        if self.bias is not None: out += self.bias
        return out

    def extra_repr(self) -> str:
        """Extra info for __repr__."""
        return (
            f"in1_features={self.in1_features}, in2_features={self.in2_features}, "
            f"out_features={self.out_features}, bias={self.bias is not None}"
        )


class LazyLinear(Module):
    """
    Linear layer with lazy initialization.

    The in_features is inferred from the first input, making it easier
    to build networks without manually calculating intermediate sizes.

    Example:
        >>> lazy = LazyLinear(256)  # out_features only
        >>> x = Tensor(np.random.randn(32, 784))  # first call infers in_features=784
        >>> y = lazy(x)  # Creates weight (256, 784) on first call
    """

    def __init__(self, out_features: int, bias: bool = True, init: str = 'xavier'):
        """
        Initialize LazyLinear layer.

        Args:
            out_features: Size of each output sample
            bias: If True, add a learnable bias
            init: Initialization method
        """
        super().__init__()
        self.out_features = out_features
        if bias:
            self.bias = Parameter(np.zeros((out_features,)))
        else:
            self.bias = None
        self.init = init
        self.in_features = None
        self.weight = None
        self._grad_buf = None

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass with lazy initialization."""
        self.in_features = x.shape[-1]
        if self.weight is None:
            self._initialize_parameters()
        return linear_op(x, self.weight, self.bias, _grad_buf=self._grad_buf)

    def _initialize_parameters(self):
        """Initialize weight and bias after in_features is known."""
        self.weight = Parameter(np.zeros((self.in_features, self.out_features)))
        self._grad_buf = np.empty((self.in_features, self.out_features), dtype=np.float32)
        if self.init == 'xavier':
            self._init_parameters(xavier_normal_)
        elif self.init == 'kaiming':
            self._init_parameters(kaiming_normal_)
        elif self.init == 'normal':
            self._init_parameters(normal_)
        elif self.init == 'zeros':
            self._init_parameters(zeros_)
        else:
            raise ValueError(f"Unknown init method: {self.init}")

    def extra_repr(self) -> str:
        in_str = self.in_features if self.in_features else "?"
        return f"in_features={in_str}, out_features={self.out_features}, bias={self.use_bias}"